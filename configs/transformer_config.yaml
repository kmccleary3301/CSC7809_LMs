# Transformer model configuration
model_type: transformer
vocab_size: 10000  # This should be replaced with the actual vocabulary size
checkpoint_path: "models"

# Model architecture
embedding_dim: 512
hidden_dim: 1024
num_layers: 4
num_heads: 8
dropout: 0.1

# Training parameters
lr: 0.0001
weight_decay: 0.00001
epochs: 50
patience: 5
batch_size: 16

seq_length: 64
batch_size: 128


# File management
save_dir: project_results/transformer 